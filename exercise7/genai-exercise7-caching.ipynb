{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5837b1",
   "metadata": {},
   "source": [
    "# Task 2: Caching Bedrock responses\n",
    "In this assignment, you will explore the Amazon Bedrock Converse API through an interactive chat session. You will learn how to cache responses and conserve tokens to reduce the overall cost of using Bedrock.\n",
    "\n",
    "## Task 2.1: Setting up Bedrock\n",
    "In the Terminal application, run the following command: jupyter notebook\n",
    "\n",
    "In the Jupyter window, select File, New, and then Notebook.\n",
    "\n",
    "Select Untitled and enter genai-exercise7-caching.ipynb as the name for your newly created notebook.\n",
    "\n",
    "Choose Rename.\n",
    "\n",
    "In the first cell, paste the following code. After you have pasted the code, select the cell, and then choose the play button at the top of the notebook to run the this block of code. This will set up the variables and connections that will be needed in future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4429ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "bedrock=boto3.client('bedrock-runtime',region_name='us-east-1')\n",
    "\n",
    "MODEL_ID = \"amazon.nova-micro-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46b8248",
   "metadata": {},
   "source": [
    "# Task 2.2: Training Bedrock\n",
    "Create a new cell in your Jupyter notebook.\n",
    "\n",
    "Paste the following code in the newly created cell. After you have pasted, select the cell, and then choose the play button at the top of the notebook to run the this block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03d0e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    " You are an assistant that summarizes music reviews for a record company.\n",
    " Here are examples:\n",
    "\n",
    " Review: The latest album by The New Wave Band is a masterpiece! Every track is a hit.\n",
    " Summary: Reviewer praises the latest album as a masterpiece with hit tracks.\n",
    "\n",
    " Review: I was disappointed with the new single; it lacked the energy of their previous work.\n",
    " Summary: Reviewer expresses disappointment, noting a lack of energy compared to previous work.\n",
    " \"\"\"\n",
    "\n",
    "user_input_review = \"This EP is a solid effort with a few standout songs, though some tracks feel repetitive.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864f4bf9",
   "metadata": {},
   "source": [
    "## Task 2.3: No caching\n",
    "Create a new cell in your Jupyter notebook.\n",
    "\n",
    "Paste the following code in the newly created cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db879a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[No Caching] Generated Summary:\n",
      "Reviewer acknowledges the EP as a solid effort with a few standout songs, but notes that some tracks feel repetitive.\n",
      "Total Tokens Used (No Cache): 128\n"
     ]
    }
   ],
   "source": [
    "# Define the payload for the API request without caching\n",
    "no_cache_payload = {\n",
    "     \"system\": [ # Start of the system prompt list\n",
    "         {\"text\": system_prompt} # The system prompt text\n",
    "     ],\n",
    "     \"messages\": [ # Start of the user messages list\n",
    "         {\n",
    "             \"role\": \"user\", # Role of the message sender\n",
    "             \"content\": [ # Content of the user message\n",
    "                 {\"text\": user_input_review + \"\\nSummary:\"} # The user's review text followed by \"Summary:\"\n",
    "             ]\n",
    "         }\n",
    "     ]\n",
    " }\n",
    "\n",
    "# Make an API call to the bedrock service using the defined payload\n",
    "no_cache_response = bedrock.converse(\n",
    "     modelId=MODEL_ID, # Specify the model ID to use\n",
    "     system=no_cache_payload[\"system\"], # Pass the system prompt from the payload\n",
    "     messages=no_cache_payload[\"messages\"] # Pass the user messages from the payload\n",
    " )\n",
    "\n",
    "# Extract the generated text output from the API response\n",
    "no_cache_output = no_cache_response['output']['message']['content'][0]['text']\n",
    "# Extract the number of input tokens used from the API response\n",
    "no_cache_input_tokens = no_cache_response['usage']['inputTokens']\n",
    "# Extract the number of output tokens generated from the API response\n",
    "no_cache_output_tokens = no_cache_response['usage']['outputTokens']\n",
    "# Calculate the total number of tokens used (input + output)\n",
    "no_cache_tokens = no_cache_input_tokens + no_cache_output_tokens\n",
    "\n",
    "# Print a header indicating the summary is without caching\n",
    "print(\"[No Caching] Generated Summary:\")\n",
    "# Print the generated summary text\n",
    "print(no_cache_output)\n",
    "# Print the total number of tokens used for this request\n",
    "print(f\"Total Tokens Used (No Cache): {no_cache_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92299d05",
   "metadata": {},
   "source": [
    "## Task 2.4: With caching\n",
    "This task will compare the token usage of Bedrock when caching responses versus the token usage of an uncached API call.\n",
    "\n",
    "Create a new cell in your Jupyter notebook.\n",
    "\n",
    "Paste the following code in the newly created cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f42d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[With Caching] Generated Summary:\n",
      "Reviewer acknowledges the EP as a solid effort with a few standout songs, but notes that some tracks feel repetitive.\n",
      "Total Tokens Used: 44\n"
     ]
    }
   ],
   "source": [
    "# Define a dictionary for caching configuration.\n",
    "# This specifies that a default cache point should be used for the current request.\n",
    "cache_point = {\"cachePoint\": {\"type\": \"default\"}}\n",
    "\n",
    "# Construct the payload for the bedrock.converse API call, including caching.\n",
    "payload_with_caching = {\n",
    "    \"system\": [\n",
    "        # The system prompt provides context or instructions to the model.\n",
    "        {\"text\": system_prompt},\n",
    "        # Integrate the cache point configuration into the system messages.\n",
    "        # This tells Bedrock to check and potentially store the response in the cache.\n",
    "        cache_point\n",
    "    ],\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                # The user's input, which includes the review to be summarized.\n",
    "                # Adding \"Summary:\" at the end prompts the model for a summary.\n",
    "                {\"text\": user_input_review + \"\\nSummary:\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Call the Amazon Bedrock Converse API with the model ID and the constructed payload.\n",
    "# This sends the prompt to the model, and due to `cache_point`, it will leverage caching.\n",
    "response = bedrock.converse(\n",
    "    modelId=MODEL_ID,\n",
    "    system=payload_with_caching[\"system\"],\n",
    "    messages=payload_with_caching[\"messages\"]\n",
    ")\n",
    "\n",
    "# Extract the generated summary text from the model's response.\n",
    "cache_output = response['output']['message']['content'][0]['text']\n",
    "# Extract the number of input tokens used for the request.\n",
    "cache_input_tokens = response['usage']['inputTokens']\n",
    "# Extract the number of output tokens generated by the model.\n",
    "cache_output_tokens = response['usage']['outputTokens']\n",
    "# Calculate the total number of tokens (input + output) used for this interaction.\n",
    "cache_tokens = cache_input_tokens + cache_output_tokens\n",
    "\n",
    "# Print a header indicating that the following output is with caching enabled.\n",
    "print(\"[With Caching] Generated Summary:\")\n",
    "# Print the summary generated by the model.\n",
    "print(cache_output)\n",
    "# Print the total number of tokens consumed by this cached interaction.\n",
    "print(f\"Total Tokens Used: {cache_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5a699",
   "metadata": {},
   "source": [
    "## Task 3: Changing your Bedrock prompt\n",
    "In this task, you'll modify your Bedrock instructions to observe how caching behaves when you change your original request.\n",
    "\n",
    "Return to the Jupyter cell that contains this text: system_prompt.\n",
    "\n",
    "Replace the content of that cell with the following text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a634230",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    " You are a pet expert that will tell the user what type of pet they have based on a description. You'll keep your responses short and generalized.\n",
    " Here are examples:\n",
    "\n",
    " Description: My pet barks and has four legs.\n",
    " Summary: Your pet is a dog.\n",
    "\n",
    " Review: My pet meows and uses a litter box.\n",
    " Summary: Your pet is a cat.\n",
    " \"\"\"\n",
    "\n",
    "user_input_review = \"My pet sings and has wings.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6a623d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[With Caching] Generated Summary:\n",
      "Your pet is a bird.\n",
      "Total Tokens Used: 17\n"
     ]
    }
   ],
   "source": [
    "cache_point = {\"cachePoint\": {\"type\": \"default\"}}\n",
    "\n",
    "payload_with_caching = {\n",
    "     \"system\": [\n",
    "         {\"text\": system_prompt},\n",
    "         cache_point\n",
    "     ],\n",
    "     \"messages\": [\n",
    "         {\n",
    "             \"role\": \"user\",\n",
    "             \"content\": [\n",
    "                 {\"text\": user_input_review + \"\\nSummary:\"}\n",
    "             ]\n",
    "         }\n",
    "     ]\n",
    " }\n",
    "\n",
    "response = bedrock.converse(\n",
    "     modelId=MODEL_ID,\n",
    "     system=payload_with_caching[\"system\"],\n",
    "     messages=payload_with_caching[\"messages\"]\n",
    " )\n",
    "\n",
    "cache_output = response['output']['message']['content'][0]['text']\n",
    "cache_input_tokens = response['usage']['inputTokens']\n",
    "cache_output_tokens = response['usage']['outputTokens']\n",
    "cache_tokens = cache_input_tokens + cache_output_tokens\n",
    "\n",
    "print(\"[With Caching] Generated Summary:\")\n",
    "print(cache_output)\n",
    "print(f\"Total Tokens Used: {cache_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8a733",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd770f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
